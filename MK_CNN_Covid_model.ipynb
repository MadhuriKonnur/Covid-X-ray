{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the CNN  model by using keras.....\n",
    "\n",
    "4-Convolution layers\n",
    "\n",
    "2-Dense for nn-hidden and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras.layers import *\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building CNN model using keras\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Conv2D(filters=32,kernel_size=(3,3),activation='relu',input_shape=(224,224,3)))\n",
    "model.add(Conv2D(64,(3,3),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(Conv2D(64,(3,3),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128,(3,3),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# now 4 convolution layers are  added\n",
    "# now adding  flatten layer for 1D input NN\n",
    "model.add(Flatten())\n",
    "\n",
    "#model.summary()\n",
    "\n",
    "#Adding  Dense or hidden layers for  our  NN (Fully connected layers)\n",
    "\n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(Dropout(0.50))\n",
    "\n",
    "# adding  output  layer with 1 output  since it is binary classification\n",
    "\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "#compling model with all layers\n",
    "\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 222, 222, 32)      896       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 220, 220, 64)      18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 110, 110, 64)      0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 110, 110, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 108, 108, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 54, 54, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 54, 54, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 52, 52, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 26, 26, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 26, 26, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 86528)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                5537856   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 5,668,097\n",
      "Trainable params: 5,668,097\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To make data ready for keras\n",
    "# Training data images of x-ray \n",
    "# Normalizing data  by rescaling size of imageand adding some augmnentation to train data like shear,zoom etc\n",
    "# by keeping horizontal_flip = True becoz not invert the x-ray\n",
    "\n",
    "train_datagen= image.ImageDataGenerator(\n",
    "               rescale=1./255,\n",
    "               shear_range=0.2,\n",
    "               zoom_range=0.2,\n",
    "               horizontal_flip=True\n",
    "               )\n",
    "\n",
    "#just applying rescaling\n",
    "\n",
    "test_dataset=image.ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "# to remove extra class indices '.ipynb_checkpoints'\n",
    "#shutil.rmtree('output/train/.ipynb_checkpoints')\n",
    "#shutil.rmtree('output/val/.ipynb_checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 231 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# actual application of generators \n",
    "# on objects  (here train_datagen)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'output/train',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    " )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Covid': 0, 'Normal': 1}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 58 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# for test dataset \n",
    "\n",
    "validation_generator= test_dataset.flow_from_directory(\n",
    "    'output/val',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Covid': 0, 'Normal': 1}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_generator.class_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.8926 - accuracy: 0.5541 - val_loss: 0.6576 - val_accuracy: 0.8103\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 26s 3s/step - loss: 0.5522 - accuracy: 0.7316 - val_loss: 0.4613 - val_accuracy: 0.9655\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 27s 3s/step - loss: 0.3687 - accuracy: 0.8312 - val_loss: 0.2701 - val_accuracy: 0.9483\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 25s 3s/step - loss: 0.2579 - accuracy: 0.9134 - val_loss: 0.1785 - val_accuracy: 0.9655\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 26s 3s/step - loss: 0.2431 - accuracy: 0.9048 - val_loss: 0.1772 - val_accuracy: 0.9655\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 26s 3s/step - loss: 0.2527 - accuracy: 0.9177 - val_loss: 0.1498 - val_accuracy: 0.9828\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 26s 3s/step - loss: 0.2178 - accuracy: 0.9091 - val_loss: 0.1404 - val_accuracy: 0.9828\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 26s 3s/step - loss: 0.1595 - accuracy: 0.9697 - val_loss: 0.1033 - val_accuracy: 0.9828\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 26s 3s/step - loss: 0.1210 - accuracy: 0.9567 - val_loss: 0.0484 - val_accuracy: 0.9828\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 26s 3s/step - loss: 0.1800 - accuracy: 0.9394 - val_loss: 0.0535 - val_accuracy: 0.9828\n"
     ]
    }
   ],
   "source": [
    "hist=model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=8,\n",
    "    epochs=10,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss is  decreasing \n",
    "\n",
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_mk.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.04488437622785568, 0.9567099809646606]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate_generator(train_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.04527151212096214, 0.982758641242981]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate_generator(validation_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here 95%  accuracy from test data .\n",
    "\n",
    "and 98% accuracy  from validation /test  data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing  the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and evaluate a saved model\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "model= load_model('model_mk.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Covid': 0, 'Normal': 1}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual= []\n",
    "y_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in os.listdir(\"./output/val/Normal/\"):\n",
    "    img= image.load_img(\"./output/val/Normal/\"+i,target_size=(224,224))\n",
    "    img= image.img_to_array(img)\n",
    "    img= np.expand_dims(img,axis=0)\n",
    "    p= model.predict_classes(img)\n",
    "    y_test.append(p[0,0])\n",
    "    y_actual.append(1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in os.listdir(\"./output/val/Covid/\"):\n",
    "    img= image.load_img(\"./output/val/Covid/\"+i,target_size=(224,224))\n",
    "    img= image.img_to_array(img)\n",
    "    img= np.expand_dims(img,axis=0)\n",
    "    p= model.predict_classes(img)\n",
    "    y_test.append(p[0,0])\n",
    "    y_actual.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual=np.array(y_actual)\n",
    "y_test= np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm= confusion_matrix(y_actual,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fd9b1688690>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVoAAAD4CAYAAACt8i4nAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAPm0lEQVR4nO3dfYxc1X3G8efxGtrKdlOogRpDwIBb6qSNQQYSIVUQXmKgiSEJLw4FE5wupTGYF7cgWkQoimIpIYCqJMpGOJjw4kDBjUMJL0FEFmkL2MEBg3EI1AV7F5tAmhgsBe/Mr3/MACt72ZndvWfu3ePvRzry7J3dMz9p7Ycf5557ryNCAIB0xpVdAADkjqAFgMQIWgBIjKAFgMQIWgBIbHzqD9j2q0PZ1oCd/MXUC8suARX04u8WerRz/Obtg9rOnA/s/tKoP68ddLQAkFjyjhYAOqreVXYFOyFoAWTFter9jzpBCyArrndk2XVYCFoAWXG97Ap2RtACyAtBCwBpuYIbSglaAFlh6QAAEnOtei0tQQsgL3S0AJCW63S0AJAWHS0ApMWuAwBIzP1lV7AzghZAXir4wFmCFkBW2EcLAKkRtACQFifDACA1OloASMs17kcLAGnR0QJAYgQtACTGyTAASItnhgFAapwMA4DEKrhGW70HoAPAaMQwxhBs72/7UdvrbD9re2Hz+Jdsb7K9pjlOblUSHS2AvBS3Rtsv6fKI+JntSZJW2364+d4NEfG1diciaAHkJYoJ2ojok9TXfL3V9jpJU0cyF0sHALLi+jCG3W171YDRPeic9oGSDpP0ePPQAttP215ie49WNRG0APJSc9sjInoiYtaA0bPjdLYnSrpH0iUR8VtJ35J0sKSZanS817cqiaUDAHkpcB+t7d3UCNnbI+JeSYqIzQPe/46k+1rNQ0cLIC/h9scQbFvSzZLWRcTXBxyfMuDbTpO0tlVJdLQA8lLcPtqjJZ0j6Rnba5rHrpI01/ZMNTaIbZB0QauJCFoAeSlu18Fjkgab7P7hzkXQAshKDGONtlMX6xK0APLCvQ4AILGClg6KRNACyAu3SQSAxOhoASAxOloASCs4GQYAibF0AACJsXQAAInR0QJAYnS0AJBWtHgWWBkIWgB5qVXv7q8ELYCsBGu0AJAYa7S7jlc3h66+Tnr9DcmWPjNH+twZ1vpfhL78Vel3b0tdXdJVi6QPz6jeXwykt/jbx+vjJ0/T669t00mH3152OfmoYEdbvcWMTHR1SZddJN17h3Vrj/T9e6UX/yd04zel7vOl7y+1LvyCdOM3y64UZbnne8/p85/897LLyE6E2x6d0rKjtX2opDlqPM88JPVKWhER6xLXNqbtNdnaa3Lj9YQJ1rQDQq+91uhu33qrcfzNt/Tu92DX8+RjvZp6wKSyy8hPcY+yKcyQQWv7CklzJS2T9ETz8H6S7rS9LCIWJ64vC719ofUvSB/+kLRoofTFy6QbvhGq16Vbvl12dUBeooK7DlpVNF/SERGxOCJua47Fko5svjco2922V9leteTW/yuy3jFn27bQon+SFl0sTZxg3b1cuvwi6YHl1qKLpWu/UnaFQGYKegpukVoFbV3SvoMcn6IhGvSI6ImIWREx6/xz/2g09Y1p2/sbIXvSidJxxzR+qff9SDrumMb7J3xceva58uoDcjQW12gvkfSI7RckvdI89kFJh0hakLKwsS4idO1XpGkHSOec9d4vdK/J0uqnpFmHS0+slj64f4lFAjkaa9u7IuIB23+qxlLBVDUeGrlR0pMRUetAfWPWmqel/3hAmn6wdOa8xjWBCy6Qrr5C+upNUn8t9Hu7S//8jyUXitLceOtsHfVX+2mPyb+vx148Xzdd97juvuXZsssa+yq4vavlroOIqEv67w7UkpXDPmI99dPB37tjSWdrQTVdcu4DZZeQJW78DQCJcQkuAKRG0AJAWjHWToYBwJhDRwsAaVVxjbZ616oBwChEzW2Podje3/ajttfZftb2wubxPW0/bPuF5p97tKqJoAWQlQKvDOuXdHlE/Lmkj0r6ou0Zkq6U9EhETJf0SPPrIRG0APJSd/tjCBHRFxE/a77eKmmdGhduzZG0tPltSyWd2qokghZAVobT0Q68AVZzdA82p+0DJR0m6XFJ+0REX+Ozok/S3q1q4mQYgLwM42RYRPRI6hnqe2xPlHSPpEsi4rf28E+2EbQAslLkrgPbu6kRsrdHxL3Nw5ttT4mIPttTJG1pNQ9LBwCyUuCuA0u6WdK6iPj6gLdWSJrXfD1P0g9a1URHCyArBXa0R0s6R9Izttc0j10labGku2zPl/SypNNbTUTQAshLQUEbEY+pcWvYwRw3nLkIWgBZ4V4HAJBYFS/BJWgBZCXq1TvHT9ACyEq872Njy0PQAsgLSwcAkBZrtACQGEELAKkRtACQVr3GrgMASCvKLmBnBC2ArLBGCwCJEbQAkBj3OgCAxLgEFwASY+kAABILdh0AQFp0tACQGifDACAtOloASKzOrgMASIuOFgBSI2gBIC0eZQMAibF0AACJEbQAkBi7DgAgNTpaAEiriksH1euxAWAUItz2aMX2EttbbK8dcOxLtjfZXtMcJ7eah6AFkJWotz/acIuk2YMcvyEiZjbH/a0mYekAQFaKPBkWESttHzjaeehoAWRlOEsHtrttrxowutv8mAW2n24uLezR6psJWgBZGU7QRkRPRMwaMHra+IhvSTpY0kxJfZKub/UDLB0AyErqXQcRsfmd17a/I+m+Vj9D0ALISuqgtT0lIvqaX54mae1Q3y91IGj/ZP/zUn8ExqBXtt5UdgmopIWjn6LAJyzYvlPSMZIm294o6RpJx9ieKSkkbZB0Qat56GgBZKXgXQdzBzl883DnIWgBZIWn4AJAYlW8BJegBZAVghYAEiNoASAxghYAEqvXqnfBK0ELICt0tACQGEELAIkRtACQGEELAInxFFwASCwKvKlMUQhaAFlh6QAAEuOmMgCQWJ2OFgDSYukAABJj1wEAJEZHCwCJsb0LABKjowWAxAhaAEiMoAWAxGrsOgCAtOhoASCxqJddwc4IWgBZoaMFgMS41wEAJMYluACQWBWXDqoX/QAwCvVw26MV20tsb7G9dsCxPW0/bPuF5p97tJqHoAWQlYj2RxtukTR7h2NXSnokIqZLeqT59ZAIWgBZibrbHi3nilgp6Y0dDs+RtLT5eqmkU1vNQ9ACyEqE2x62u22vGjC62/iIfSKir/FZ0Sdp71Y/wMkwAFmpDeM2iRHRI6knXTUNBC2ArHRg18Fm21Mios/2FElbWv0ASwcAslLkroP3sULSvObreZJ+0OoH6GgBZKXIx43bvlPSMZIm294o6RpJiyXdZXu+pJclnd5qHoIWQFaKXDqIiLnv89Zxw5mHoAWQlVqteleGEbQAslLFS3AJWgBZ4e5dAJBYkSfDikLQAsgKHS0AJEZHCwCJDecS3E4haAFkhY4WABJjjRYAEqtiR8tNZTrk+BOmafXP/1Zr1l6gSxd9tOxyUILNr0oXnt+lMz41XmeeOl7Lbmv88/vFeun8s7s097TxumxBl958s+RCx7iCn7BQCDraDhg3zrr+xhM155Rl2rRpq37y2Hm6/74XtP7518suDR3U1SUtXFTToTOkt96Szj1zvI78WF1fvqZLCy+v6/AjQiuWW7d9d5z+7qJ62eWOWVVcOqCj7YBZR0zRSy/+Whs2/Ebbt9d1z93P6ZS/nl52WeiwyXtJh85ovJ4wQZo2LfTaZuvlDdZhsxrt1VEfCz36Y/5ZjkYt2h+dwm+0A6bsO0kbN2599+veTVu179RJJVaEsvVuktY/b33oL0MHHRJa+WijC/vxg+O0+dWSixvjQm57dMqIg9b254d4793n8Lzd/8RIPyIbHuT3WcUFe3TGtm3SlZeO12VX1DRxonT1v9T0b8vG6dwzxmvbNmn8bmVXOLbVo/3RKaNZo71W0ncHe2Pgc3j+8A8W7/KR0rtpq/bb770Odt+pk9TXu3WIn0Cu+rdLV1zapU+cUtexxzf+aRx4kPSvPTVJ0v9ukH66snprjGNJFQNnyKC1/fT7vSVpn+LLydPqVX066JA9dcABH1Bv71Z95vQZmn/eirLLQodFSNdd06VpB4XOnvfeya43Xpf2/GOpXpeW9HTp02dwImw0OtmptqtVR7uPpE9I+vUOxy3pP5NUlKFaLfQPlz6k5T88U11d1veWPq3n1/2q7LLQYT9/yvrRD8fpkOmhsz/bWLX7+4treuVl6+5lja+PPa6uT55awaQYQzp5kqtdrYL2PkkTI2LNjm/Y/kmSijL10IMv6aEHkz/VGBU28/DQE89sH+Sd0Fl/QxdblArm7NBBGxHzh3jvc8WXAwCjU8X/ZHHBAoCsjLmOFgDGGjpaAEisinvUCVoAWamVXcAgCFoAWWHpAAASI2gBILEKLtEStADyQkcLAIlFgT2t7Q2Stqpxjq0/ImaNZB6CFkBWEuw6ODYiRnVzEoIWQFaquHTAExYAZCUcbY+BDyloju4dp5P0kO3Vg7zXNjpaAFkZTkc78CEF7+PoiOi1vbekh20/HxErh1sTHS2ArNSHMVqJiN7mn1skLZd05EhqImgBZKWmaHsMxfYE25PeeS3pRElrR1ITSwcAslLg9q59JC134+mq4yXdEREPjGQighZAVoradRARL0n6SBFzEbQAshIVfIgwQQsgK/UK3u2AoAWQlSpesEDQAshKq90EZSBoAWSFpQMASIyTYQCQGB0tACRW5P1oi0LQAsgKuw4AIDF2HQBAYnUTtACQFCfDACCx6sUsQQsgM3S0AJBYP0ELAGmxjxYAEmPpAAASY3sXACTGlWEAkBhLBwCQWK2CPS1BCyArdLQAkBhBCwCJEbQAkFidR9kAQFp0tACQ2HZ2HQBAWnS0AJBYFYN2XNkFAECRaq63PVqxPdv2etu/tH3lSGuiowWQlaIezmi7S9I3JJ0gaaOkJ22viIjnhjsXQQsgK2+30am26UhJv4yIlyTJ9jJJcyQNO2gdUb31jFzZ7o6InrLrQLXw96I8trsldQ841PPO78L2ZyXNjogvNL8+R9JREbFguJ/DGm1ndbf+FuyC+HtRkojoiYhZA8bA/+ANdunDiDpTghYABrdR0v4Dvt5PUu9IJiJoAWBwT0qabnua7d0lnSVpxUgm4mRYZ7EOh8Hw96KCIqLf9gJJD0rqkrQkIp4dyVycDAOAxFg6AIDECFoASIyg7ZCiLuVDPmwvsb3F9tqya0FaBG0HDLiU7yRJMyTNtT2j3KpQAbdIml12EUiPoO2Mdy/li4i3Jb1zKR92YRGxUtIbZdeB9Ajazpgq6ZUBX29sHgOwCyBoO6OwS/kAjD0EbWcUdikfgLGHoO2Mwi7lAzD2ELQdEBH9kt65lG+dpLtGeikf8mH7Tkn/JenPbG+0Pb/smpAGl+ACQGJ0tACQGEELAIkRtACQGEELAIkRtACQGEELAIkRtACQ2P8DiHSbuihOS5oAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.heatmap(cm,cmap='plasma',annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
